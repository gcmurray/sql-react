import chromadb
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
import json
import hashlib
from langchain_core.documents import Document
from collections import defaultdict
import random
from clusters import *

def stratified_sample_equal(by_db, n_per_db, seed=None):
    if seed is not None:
        random.seed(seed)

    sample = []
    for db, items in by_db.items():
        k = min(n_per_db, len(items))
        if k == 0:
            k = len(items)
        sample.extend(random.sample(items, k))
    return sample

class ExemplarStore():
    def __init__(self, train_path, dblist=[], num_examples=0, method="random-stratified"):
        self.train_path = train_path
        self.dblist = dblist
        self.num_examples = num_examples
        #create an ephemeral client for the purposes of creating a exemplar vector store
        self.vector_client = chromadb.EphemeralClient()
        
        # embeddings for vector store (play with these?)
        self.embeddings = OllamaEmbeddings(model="nomic-embed-text")

        # the vector store for nl query, with sql metadata.
        self.nl_store = Chroma(
            collection_name="nl-sql",
            embedding_function=self.embeddings,
            client=self.vector_client,
        )

        # the vector store for sql query, with nl metadata.
        self.sql_store = Chroma(
            collection_name="nl-sql",
            embedding_function=self.embeddings,
            client=self.vector_client,
        )
        
        self._init_data(method=method)
    
    def _init_data(self, method):
        # load json data
        json_sql_data = None # json data is a list of dicts
        with open(self.train_path, "r") as f: 
            json_sql_data = json.load(f)

        #if no subset, add all subsets to set.
        if self.dblist == []:
            dbset = set()
            for item in json_sql_data:
                dbset.add(item["db_id"])
            self.dblist = list(dbset)

        # filter the json data samples down to the listed databases.
        filtered = list(filter(lambda d: d['db_id'] in self.dblist, json_sql_data))

        # setup stratified samples by database. This portion generated by prompting ChatGPT:
        ##############################################################################################
        # i have a list of dictionaries each representing an SQL query. Each dict has a database key, which is one of a number of database names. I can filter this list to the databases I want, however, now I want to be able to select examples in a stratified way across the remaining databases. How can I do this in python?

        by_db = defaultdict(list)
        for q in filtered:
            by_db[q["db_id"]].append(q)
        
        exemplars = []

        # select from all databases evenly selecting up to same number.
        if method == "random-stratified":
            exemplars = stratified_sample_equal(by_db, n_per_db=self.num_examples, seed=42)
        
        ###############################################################################################
        # mix up and draw from all added databases
        elif method == "random":
            random.shuffle(filtered)
            if self.num_examples > 0:
                if len(filtered) < self.num_examples:
                    self.num_examples = len(filtered)
            else:
                self.num_examples = len(filtered)
            
            # cut off the number of examples here
            exemplars = filtered[:self.num_examples]
        else:
            raise SystemExit("method must be either random-stratified or random.")
        
        print("number of total exemplars: {}".format(len(exemplars)))
        
        nl_docs = []
        sql_docs = []
        nl_hashes = set()
        sql_hashes = set()

        for example in exemplars:
            nl = example["question"]
            sql = example["query"]
       
            nl_hash = hashlib.sha256(nl.encode('utf-8')).hexdigest()
            sql_hash = hashlib.sha256(sql.encode('utf-8')).hexdigest()

            # if we use the hash as id, we only keep single example of this exact question.
            if nl_hash not in nl_hashes:
                nl_docs.append(Document(page_content=nl, metadata={"query": str(sql), "hash": str(nl_hash)}, id=nl_hash))
                nl_hashes.add(nl_hash)
            
            if sql_hash not in sql_hashes:
                sql_docs.append(Document(page_content=sql, metadata={"query": str(nl), "hash": str(sql_hash)}, id=sql_hash))
                sql_hashes.add(sql_hash)

        self.nl_store.add_documents(documents=nl_docs)
        self.sql_store.add_documents(documents=sql_docs)
    
    def get_similar_nl(self, query, k=1):
        return self.nl_store.similarity_search(query, k=k)
    
    def get_similar_sql(self, query, k=1):
        return self.sql_store.similarity_search(query, k=k)
    
    def get_similar(self, query, sql=True, k=1):
        if sql:
            lookup = self.get_similar_nl(query, k=1)
            sql_text = lookup[0].metadata.get('query')
            sql_return = self.get_similar_sql(sql_text, k=k)
            return sql_return
        else:
            lookup = self.get_similar_nl(query, k=k)
            return lookup


class ClusterExemplarStore():
    def __init__(self, train_path, num_clusters=5, num_neighbors=2):
        self.train_path = train_path
        self.num_clusters = num_clusters
        self.num_neighbors = num_neighbors
        #create an ephemeral client for the purposes of creating a exemplar vector store
        self.vector_client = chromadb.EphemeralClient()

        self.embeddings = OllamaEmbeddingsNormalized(model="nomic-embed-text")

        # the vector store for nl query, with sql metadata.
        self.nl_store = Chroma(
            collection_name="nl-store",
            embedding_function=self.embeddings,
            client=self.vector_client,
        )

        # the vector store for sql query, with nl metadata.
        self.sql_store = Chroma(
            collection_name="sql-store",
            embedding_function=self.embeddings,
            client=self.vector_client,
        )
        
        self._init_data()
    
    def _init_data(self):
        examples = load_examples(self.train_path)

        representatives = select_sql_representatives(examples, sql_clusters_per_db=self.num_clusters, k_neighbors=self.num_neighbors)
        print("number of total cluster exemplars: {}".format(len(representatives)))

        nl_docs = []
        sql_docs = []
        nl_hashes = set()
        sql_hashes = set()

        for e in representatives:
            nl = e.nl
            sql = e.sql
       
            nl_hash = hashlib.sha256(nl.encode('utf-8')).hexdigest()
            sql_hash = hashlib.sha256(sql.encode('utf-8')).hexdigest()

            # if we use the hash as id, we only keep single example of this exact question.
            if nl_hash not in nl_hashes:
                nl_docs.append(Document(page_content=nl, metadata={"query": str(sql), "hash": str(nl_hash)}, id=nl_hash))
                nl_hashes.add(nl_hash)
            
            if sql_hash not in sql_hashes:
                sql_docs.append(Document(page_content=sql, metadata={"query": str(nl), "hash": str(sql_hash)}, id=sql_hash))
                sql_hashes.add(sql_hash)
        
        print("number of unique nl queries: {}\n".format(len(nl_hashes)))
        print("number of unique sql queries: {}\n".format(len(sql_hashes)))
        self.nl_store.add_documents(documents=nl_docs)
        self.sql_store.add_documents(documents=sql_docs)

    def get_similar_nl(self, query, k=1):
        return self.nl_store.similarity_search(query, k=k)
    
    def get_similar_sql(self, query, k=1):
        return self.sql_store.similarity_search(query, k=k)
    
    def get_similar(self, query, sql=True, k=1):
        if sql:
            lookup = self.get_similar_nl(query, k=1)
            sql_text = lookup[0].metadata.get('query')
            sql_return = self.get_similar_sql(sql_text, k=k)
            return sql_return
        else:
            lookup = self.get_similar_nl(query, k=k)
            return lookup